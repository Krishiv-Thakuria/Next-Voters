{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a07b9a12",
   "metadata": {},
   "source": [
    "# **Political Concept Classifer Fine Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbcb307",
   "metadata": {},
   "source": [
    "### The Logic \n",
    "\n",
    "1. We will be using the sloth to finetune the model \n",
    "2. PEFT will allow us to add LoRA adapters which will allow us to finetune the model on a smaller dataset\n",
    "3. TRT will allow us to add the techinical configurations needed\n",
    "4. We will download the GUFF file and save it in the political_concept_classifer folder\n",
    "5. Upload the downloaded model to Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a78be3",
   "metadata": {},
   "source": [
    "**You should run it on Google Colab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2f4603",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a804d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'immigration', 'text': 'Border security is national security - we need complete operational control of our southern border.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file = json.load(open(\"training.json\", \"r\"))\n",
    "print(file[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81d371",
   "metadata": {},
   "source": [
    "### Install all dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d9fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip uninstall -y unsloth peft\n",
    "%pip install unsloth trl peft accelerate bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9553ad",
   "metadata": {},
   "source": [
    "### Load pretrained model (without fine tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6602ed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model_name = \"unsloth/Phi-3-mini-4k-instruct-bnb-4bit\"\n",
    "\n",
    "max_seq_length = 2048  # Choose sequence length\n",
    "dtype = None  # Auto detection\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25e6a18",
   "metadata": {},
   "source": [
    "### Prepare dataset for finetuning\n",
    "Ensure that the LLM knows what is expected input and output by specifying the format of the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cec946e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def format_prompt(example):\n",
    "    return f\"### Input: {example['text']}\\n### Output: {example['category']}<|endoftext|>\"\n",
    "\n",
    "formatted_data = [format_prompt(item) for item in file]\n",
    "dataset = Dataset.from_dict({\"text\": formatted_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfb2501",
   "metadata": {},
   "source": [
    "### Add LoRA adapters \n",
    "LoRA adapters is a techinique to add a small amount of parameters to the model to improve its performance on a specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6963e3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=64,  # LoRA rank - higher = more capacity, more memory\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_alpha=128,  # LoRA scaling factor (usually 2x rank)\n",
    "    lora_dropout=0,  # Supports any, but = 0 is optimized\n",
    "    bias=\"none\",     # Supports any, but = \"none\" is optimized\n",
    "    use_gradient_checkpointing=\"unsloth\",  # Unsloth's optimized version\n",
    "    random_state=3407,\n",
    "    use_rslora=False,  # Rank stabilized LoRA\n",
    "    loftq_config=None, # LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89464a4d",
   "metadata": {},
   "source": [
    "### Add technical configurations to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1732882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments optimized for Unsloth\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,  # Effective batch size = 8\n",
    "        warmup_steps=10,\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=25,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        dataloader_pin_memory=False,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97f284a",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ad200",
   "metadata": {},
   "source": [
    "### Download the model for local use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6210b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_gguf(\"gguf_model\", tokenizer, quantization_method=\"q4_k_m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e886a9",
   "metadata": {},
   "source": [
    "### Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f77b3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi()\n",
    "token = \"hf_**TOKEN**\"\n",
    "\n",
    "# 1. Create repo\n",
    "api.create_repo(\n",
    "    repo_id=\"hemitpatel/political_concept_classifier\", # Changed 'name' to 'repo_id'\n",
    "    repo_type=\"model\",\n",
    "    private=False,\n",
    "    token=token\n",
    ")\n",
    "\n",
    "# 2. Upload your saved folder\n",
    "api.upload_folder(\n",
    "    folder_path=\"political_concept_classifier\",\n",
    "    repo_id=\"hemitpatel/political_concept_classifier\",  # IMPORTANT: replace YOUR_USERNAME with your actual Hugging Face username\n",
    "    repo_type=\"model\",\n",
    "    token=token\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
